Selected Tasks: ['arc_challenge', 'arc_easy', 'boolq', 'hellaswag', 'openbookqa', 'piqa', 'winogrande']
Task: arc_challenge; number of docs: 1172
Task: arc_challenge; document 0; context prompt (starting on next line):
Question: Cities control the amount of pollution that is allowed to come from cars. How does this most likely help people?
Answer:
(end of prompt on previous line)
Requests: [Req_loglikelihood('Question: Cities control the amount of pollution that is allowed to come from cars. How does this most likely help people?\nAnswer:', ' The air stays cleaner.')[0]
, Req_loglikelihood('Question: Cities control the amount of pollution that is allowed to come from cars. How does this most likely help people?\nAnswer:', ' Cars can travel at faster speeds.')[0]
, Req_loglikelihood('Question: Cities control the amount of pollution that is allowed to come from cars. How does this most likely help people?\nAnswer:', ' The skills of the drivers improve.')[0]
, Req_loglikelihood('Question: Cities control the amount of pollution that is allowed to come from cars. How does this most likely help people?\nAnswer:', ' It becomes safer to drive on the roads.')[0]
]
Task: arc_easy; number of docs: 2376
Task: arc_easy; document 0; context prompt (starting on next line):
Question: Which is the function of the gallbladder?
Answer:
(end of prompt on previous line)
Requests: [Req_loglikelihood('Question: Which is the function of the gallbladder?\nAnswer:', ' store bile')[0]
, Req_loglikelihood('Question: Which is the function of the gallbladder?\nAnswer:', ' produce bile')[0]
, Req_loglikelihood('Question: Which is the function of the gallbladder?\nAnswer:', ' store digestive enzymes')[0]
, Req_loglikelihood('Question: Which is the function of the gallbladder?\nAnswer:', ' produce digestive enzymes')[0]
]
Task: boolq; number of docs: 3270
Task: boolq; document 0; context prompt (starting on next line):
NCIS: New Orleans (season 4) -- The fourth season of NCIS: New Orleans premiered on September 26, 2017 on CBS. The series continues to air following Bull, Tuesday at 10:00 p.m. (ET) and contained 24 episodes. The season concluded on May 15, 2018.
Question: is ncis new orleans over for the season?
Answer:
(end of prompt on previous line)
Requests: (Req_loglikelihood('NCIS: New Orleans (season 4) -- The fourth season of NCIS: New Orleans premiered on September 26, 2017 on CBS. The series continues to air following Bull, Tuesday at 10:00 p.m. (ET) and contained 24 episodes. The season concluded on May 15, 2018.\nQuestion: is ncis new orleans over for the season?\nAnswer:', ' yes')[0]
, Req_loglikelihood('NCIS: New Orleans (season 4) -- The fourth season of NCIS: New Orleans premiered on September 26, 2017 on CBS. The series continues to air following Bull, Tuesday at 10:00 p.m. (ET) and contained 24 episodes. The season concluded on May 15, 2018.\nQuestion: is ncis new orleans over for the season?\nAnswer:', ' no')[0]
)
Task: hellaswag; number of docs: 10042
Task: hellaswag; document 0; context prompt (starting on next line):
Personal Care and Style: How to increase breast size with a bra. Check your bra size. Wearing a bra that is too big will not make your breasts look larger. That is why it is important to wear the right size bra for you.
(end of prompt on previous line)
Requests: [Req_loglikelihood('Personal Care and Style: How to increase breast size with a bra. Check your bra size. Wearing a bra that is too big will not make your breasts look larger. That is why it is important to wear the right size bra for you.', ' You can visit a lingerie shop and have them measure you to help you fit a bra to your size, or measure yourself before you shop for a new bra to ensure that you get a good fit. Use a flexible tape measure, like one found in a sewing kit.')[0]
, Req_loglikelihood('Personal Care and Style: How to increase breast size with a bra. Check your bra size. Wearing a bra that is too big will not make your breasts look larger. That is why it is important to wear the right size bra for you.', ' This is why it is important to keep your breasts under protection when in the shower and only wear bras that are larger than your breast size. If you are not wearing a bra, try wearing something that is a little bigger.')[0]
, Req_loglikelihood('Personal Care and Style: How to increase breast size with a bra. Check your bra size. Wearing a bra that is too big will not make your breasts look larger. That is why it is important to wear the right size bra for you.', ' For a girl, a bra with a support strap will be easier for her, because most women are unable to pull through bra straps and bras that are too small will not be able to support breasts from side-to-side. Many bras have even been created that cover the breast side, and can be sent to other women in the world to make them look bigger.')[0]
, Req_loglikelihood('Personal Care and Style: How to increase breast size with a bra. Check your bra size. Wearing a bra that is too big will not make your breasts look larger. That is why it is important to wear the right size bra for you.', ' Choose a color that is flattering to your breast type and specific event, in addition to those that make you uncomfortable. Look for sports bras made from natural material, such as spandex or lycra, as this is a more breathable bra.')[0]
]
Task: openbookqa; number of docs: 500
Task: openbookqa; document 0; context prompt (starting on next line):
Atomic 26 is drawn to a device, it could be
(end of prompt on previous line)
Requests: [Req_loglikelihood('Atomic 26 is drawn to a device, it could be', ' magnetized')[0]
, Req_loglikelihood('Atomic 26 is drawn to a device, it could be', ' Na')[0]
, Req_loglikelihood('Atomic 26 is drawn to a device, it could be', ' compass')[0]
, Req_loglikelihood('Atomic 26 is drawn to a device, it could be', ' K')[0]
]
Task: piqa; number of docs: 1838
Task: piqa; document 0; context prompt (starting on next line):
Question: Remove seeds from  strawberries
Answer:
(end of prompt on previous line)
Requests: [Req_loglikelihood('Question: Remove seeds from  strawberries\nAnswer:', ' Blend the strawberries, pour the mixture through a fine-mesh strainer with a bowl underneath to catch the pulps and strain out the seeds')[0]
, Req_loglikelihood('Question: Remove seeds from  strawberries\nAnswer:', ' Chop up the strawberries, pour the mixture through a fine-mesh strainer with a bowl underneath to catch the pulps and strain out the seeds')[0]
]
Task: winogrande; number of docs: 1267
Task: winogrande; document 0; context prompt (starting on next line):
People think Rebecca
(end of prompt on previous line)
Requests: [Req_loglikelihood('People think Samantha', ' is embarassed, because Samantha made snide comments about the shirt Rebecca was wearing.')[0]
, Req_loglikelihood('People think Rebecca', ' is embarassed, because Samantha made snide comments about the shirt Rebecca was wearing.')[0]
]
Running loglikelihood requests
{
  "results": {
    "arc_challenge": {
      "acc": 0.43430034129692835,
      "acc_stderr": 0.01448470304885736,
      "acc_norm": 0.46245733788395904,
      "acc_norm_stderr": 0.014570144495075578
    },
    "arc_easy": {
      "acc": 0.7634680134680135,
      "acc_stderr": 0.008719840797175742,
      "acc_norm": 0.7462121212121212,
      "acc_norm_stderr": 0.008929657065808292
    },
    "boolq": {
      "acc": 0.7773700305810397,
      "acc_stderr": 0.007276093141006333
    },
    "hellaswag": {
      "acc": 0.5715992830113523,
      "acc_stderr": 0.00493835661595542,
      "acc_norm": 0.7598088030272854,
      "acc_norm_stderr": 0.004263263933601566
    },
    "openbookqa": {
      "acc": 0.314,
      "acc_stderr": 0.020776701920308997,
      "acc_norm": 0.442,
      "acc_norm_stderr": 0.02223197069632112
    },
    "piqa": {
      "acc": 0.780739934711643,
      "acc_stderr": 0.009653357463605317,
      "acc_norm": 0.7910772578890098,
      "acc_norm_stderr": 0.00948522703010509
    },
    "winogrande": {
      "acc": 0.6906077348066298,
      "acc_stderr": 0.012991329330823007
    }
  },
  "versions": {
    "arc_challenge": 0,
    "arc_easy": 0,
    "boolq": 1,
    "hellaswag": 0,
    "openbookqa": 0,
    "piqa": 0,
    "winogrande": 0
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=meta-llama/Llama-2-7b-hf",
    "num_fewshot": 0,
    "batch_size": null,
    "batch_sizes": [],
    "device": "cuda",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}
hf-causal-experimental (pretrained=meta-llama/Llama-2-7b-hf), limit: None, provide_description: False, num_fewshot: 0, batch_size: None
|    Task     |Version| Metric |Value |   |Stderr|
|-------------|------:|--------|-----:|---|-----:|
|arc_challenge|      0|acc     |0.4343|±  |0.0145|
|             |       |acc_norm|0.4625|±  |0.0146|
|arc_easy     |      0|acc     |0.7635|±  |0.0087|
|             |       |acc_norm|0.7462|±  |0.0089|
|boolq        |      1|acc     |0.7774|±  |0.0073|
|hellaswag    |      0|acc     |0.5716|±  |0.0049|
|             |       |acc_norm|0.7598|±  |0.0043|
|openbookqa   |      0|acc     |0.3140|±  |0.0208|
|             |       |acc_norm|0.4420|±  |0.0222|
|piqa         |      0|acc     |0.7807|±  |0.0097|
|             |       |acc_norm|0.7911|±  |0.0095|
|winogrande   |      0|acc     |0.6906|±  |0.0130|

results/Llama-2-7b-hf/zeroshot_acc.csv
